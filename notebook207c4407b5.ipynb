{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## importations\nimport random\nimport pandas as pd\nimport gc\nimport pickle\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pipeline variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## data loading\ny2014=pd.read_excel(\"../input/monoprix/2014.xlsx\")\ny2015=pd.read_excel(\"../input/monoprix/2015.xlsx\")\ny2016=pd.read_excel(\"../input/monoprix/2016.xlsx\")\ny2017=pd.read_excel(\"../input/monoprix/2017.xlsx\")\ny2018=pd.read_excel(\"../input/monoprix/2018.xlsx\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lis=[y2014,y2015,y2016,y2017,y2018]\ndf=pd.concat(lis,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_items=[783480746,611080027,708080011,437000050,646760010,790880881,783480696]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx=df[[\"item_id\",\"id_struct\"]].drop_duplicates([\"item_id\",\"id_struct\"]).set_index([\"item_id\"]).loc[l_items,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yy=df[[\"item_id\",\"id_struct\",\"item_cnt_day\"]].sort_values(by='item_cnt_day', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data preprocessing\nl=[]\nold=1\ncount=0\nfor i in df.Date:\n    if float(i.month)!=float(old):\n        old=float(i.month)\n        count=count+1\n    l.append(count)\ndf[\"date_block_num\"]=l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## EDA\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    print(col,df[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from pandas_profiling import ProfileReport\n#profile = ProfileReport(df.sample(frac=0.01), title=\"Training Data Profile\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df[['Date','date_block_num','shop_id','item_id','item_category','id_struct','Price','item_cnt_day']]\nx.set_index(['shop_id', 'item_id'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_possibilities=list(set(list(x.index)))\n#457","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair=random.choice(list_of_possibilities)\nxx=x.loc[pair]\nplt.plot(xx.item_cnt_day.values,xx.Date.values)#'--bo'\nplt.ylabel(str(pair))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx=x.loc[pair]\nplt.plot(xx.Price.values,xx.Date.values)#'--bo'\nplt.ylabel(str(pair))\nprint(pair)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx=x.loc[pair]\nplt.plot(xx.Price.values,xx.Date.values)#'--bo'\nplt.ylabel(str((74, 790880281)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#previous value benchmark\nX = df[[\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_day\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nX[\"item_cnt_month\"] = X.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"item_cnt_day\"].transform(np.sum)\ndel X[\"item_cnt_day\"]\nX.sort_values(by=\"item_cnt_month\",ascending= False)\nprint(len(X))\n\nX = X.drop_duplicates([\"date_block_num\", \"shop_id\", \"item_id\"])\nprint(len(X))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_month = X[\"date_block_num\"] == 42\nlast_month_X = X[last_month].copy()\nlast_month_X = last_month_X.sort_values(by=[\"shop_id\", \"item_id\"])\n\nlast_last_month = X[\"date_block_num\"] == 41\nlast_last_month_X = X[last_last_month].copy()\nlast_last_month_X = last_last_month_X.sort_values(by=[\"shop_id\", \"item_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.merge(last_month_X,last_last_month_X , on=[\"shop_id\", \"item_id\"], how=\"left\")\ncombined = combined.fillna(0)\ncombined\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bench_data=combined[[\"item_cnt_day_x\",\"item_cnt_day_y\"]]\nfrom sklearn.metrics import mean_squared_error\nimport math\nmse=mean_squared_error(bench_data.item_cnt_day_x,bench_data.item_cnt_day_y)\nrmse = math.sqrt(mse)\nprint(rmse)\n#previous value benchmark=9.43\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## data transformation and splitiing and feature engineering\n'''\ndf[\"item_cnt_month\"] = df.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"item_cnt_day\"].transform(np.sum)\ndf[\"Price_agg\"] = df.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"Price\"].transform(np.mean)\ndel df[\"item_cnt_day\"]\ndel df[\"Price\"]\ndf = df.drop_duplicates([\"date_block_num\", \"shop_id\", \"item_id\"])\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daata=df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add empty dates\nstart=daata.Date.values[0]\nend=daata.Date.values[-1]\nl=str(start).split('-')\nll=str(end).split('-')\nstart=l[0]+\"-\"+l[1]+\"-\"+l[2][:2]\nend=ll[0]+\"-\"+ll[1]+\"-\"+ll[2][:2]\nd={'Date':pd.date_range(start=start,end=end)}\ntemp_df = pd.DataFrame(data=d)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=daata[[\"shop_id\",\"item_id\",\"item_category\",\"id_struct\"]].drop_duplicates([\"shop_id\",\"item_id\"]).reset_index()\n#some items are labeled under more than one id_struct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=temp.drop([\"index\"],axis=1)\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1=daata.drop_duplicates([\"item_id\"],keep='last').reset_index()\ntemp2=temp1[[\"item_id\",\"Price\"]].set_index([\"item_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['key'] = 0\ntemp_df['key'] = 0\n\ndf_cartesian = temp_df.merge(temp, how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combination=df_cartesian.drop([\"key\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combination","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temppp=daata.drop([\"date_block_num\",\"item_category\",\"id_struct\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_comb=pd.merge(combination,temppp , on=[\"Date\",\"shop_id\",\"item_id\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_comb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lis=list(final_comb.Price.fillna(-1).values)\nlis1=list(final_comb.item_id.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kk=[lis[x] if lis[x]!=-1 else float(temp2.loc[lis1[x]]) for x in range(len(lis))]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final=final_comb.drop([\"Price\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final[\"Price\"]=kk\nfinal=final.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df.reset_index(inplace=True)\ntemp_df=temp_df.drop([\"key\"],axis=1)\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finals=pd.merge(final,temp_df , on=[\"Date\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generating new features\nhol_df=pd.read_csv(\"../input/tunisian-holidays-2014-2019/holidays.csv\")\nhol_df[\"date\"] = hol_df[\"year\"].astype(str).str.cat(hol_df[[\"month\", \"day_of_month\"]].astype(str), sep=\"-\")\nhol_df[\"Date\"]=pd.to_datetime(hol_df[\"date\"])\nhol_df=hol_df.drop([\"date\",\"year\",\"month\",\"day_of_month\"],axis=1)\nhol_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#las=list(final.Date.values)\n#las1=list(hol_df.Date.values)\n#las01=[1 if x in las1 else 0 for x in las]\nhol_df[\"key\"]=1\nfinals1=pd.merge(finals,hol_df , on=[\"Date\"], how=\"left\")\nfinals1=finals1.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finals1[\"holidays\"] = finals1[\"holiday\"].astype('category')\nfinals1[\"holidayz\"] = finals1[\"holidays\"].cat.codes\nfinals2=finals1.drop([\"holidays\",\"holidayz\",\"holiday\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finals2[\"weekend\"]=finals2.Date.dt.dayofweek\n#finals2[\"weekend\"]=finals2[[\"weekend\"]].replace({6:1,5:1,4:0,3:0,2:0,1:0})\nfinals2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#monthly_Agg\nl=[]\nold=1\ncount=0\nfor i in finals2.Date:\n    if float(i.month)!=float(old):\n        old=float(i.month)\n        count=count+1\n    l.append(count)\nfinals2[\"date_block_num\"]=l\nfinals2[\"item_cnt_month\"] = finals2.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"item_cnt_day\"].transform(np.sum)\nfinals2[\"Price_agg\"] = finals2.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"Price\"].transform(np.mean)\nfinals2[\"keyz\"] = finals2.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"key\"].transform(np.sum)\ndel finals2[\"item_cnt_day\"]\ndel finals2[\"Price\"]\ndel finals2[\"index\"]\ndel finals2[\"key\"]\n\nfinals2 = finals2.drop_duplicates([\"date_block_num\", \"shop_id\", \"item_id\"])\nfinals2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finals2.reset_index(inplace=True)\nfinals2.drop([\"index\"],axis=1,inplace=True)\nfinals2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleaning outliers\n#plot sales\nimport matplotlib.pyplot as plt\nplt.plot(df.item_cnt_day)\nplt.show()\n###leeave it for now but talk about it in the meeting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature engineering lag roll expand\ngrouped_df = finals2.groupby([\"shop_id\", \"item_id\"])\ndf_list=[]\nfor key,item in grouped_df:\n    df_g = grouped_df.get_group(key)\n    #lagg features for price and count\n    #count\n    df_g[\"item_cnt_month_lag1\"]=df_g.item_cnt_month.shift(1).fillna(0)\n    df_g[\"item_cnt_month_lag2\"]=df_g.item_cnt_month.shift(2).fillna(0)\n    df_g[\"item_cnt_month_lag3\"]=df_g.item_cnt_month.shift(3).fillna(0)\n    df_g[\"item_cnt_month_lag4\"]=df_g.item_cnt_month.shift(4).fillna(0)\n    df_g[\"item_cnt_month_lag5\"]=df_g.item_cnt_month.shift(5).fillna(0)\n    df_g[\"item_cnt_month_lag6\"]=df_g.item_cnt_month.shift(6).fillna(0)\n    df_g[\"item_cnt_month_lag7\"]=df_g.item_cnt_month.shift(7).fillna(0)\n    #df_g[\"item_cnt_month_lag8\"]=df_g.item_cnt_month.shift(8).fillna(method='bfill')\n    #price\n    df_g[\"Price_agg_lag1\"]=df_g.Price_agg.shift(1).fillna(0)\n    df_g[\"Price_agg_lag2\"]=df_g.Price_agg.shift(2).fillna(0)\n    #df_g[\"Price_agg_lag3\"]=df_g.Price_agg.shift(3).fillna(method='bfill')\n    #df_g[\"Price_agg_lag4\"]=df_g.Price_agg.shift(4).fillna(method='bfill')\n    #rolling window features for count and price\n    #count\n    #df_g[\"item_cnt_month_rol1\"]=df_g.item_cnt_month.shift(1).rolling(window=2).mean().fillna(method='bfill')\n    #price\n    #df_g[\"Price_agg_rol1\"]=df_g.Price_agg.shift(1).rolling(window=2).mean().fillna(method='bfill')\n    #df_g[\"Price_agg_rol2\"]=df_g.Price_agg.shift(2).rolling(window=3).mean().fillna(method='bfill')\n    #df_g[\"Price_agg_rol3\"]=df_g.Price_agg.shift(3).rolling(window=4).mean().fillna(method='bfill')\n    #df_g[\"Price_agg_rol4\"]=df_g.Price_agg.shift(4).rolling(window=5).mean().fillna(method='bfill')\n    #expanding features for price and count \n    #count\n    #df_g[\"item_cnt_month_exp1\"]=df_g.item_cnt_month.shift(1).expanding().mean().fillna(method='bfill')\n    #price\n    #df_g[\"Price_agg_exp1\"]=df_g.Price_agg.shift(1).expanding().mean().fillna(method='bfill')\n    df_list.append(df_g)\nnew_df=pd.concat(df_list,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.reset_index(inplace=True)\nnew_df.drop([\"index\"],axis=1,inplace=True)\nnew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0=new_df[[\"date_block_num\",\"shop_id\",\"item_id\",\"id_struct\",\"item_category\",\"Price_agg\",\"keyz\",\"item_cnt_month_lag1\",\"item_cnt_month_lag2\",\"item_cnt_month_lag3\",\"item_cnt_month_lag4\",\"item_cnt_month_lag5\",\"item_cnt_month_lag6\",\"item_cnt_month_lag7\",\"Price_agg_lag1\",\"Price_agg_lag2\"]]\ndf1=new_df[[\"item_cnt_month\"]]\n#\"date_block_num\",\"shop_id\",\"item_id\",\"id_struct\",\"item_category\",\"Price_agg\",\"keyz\",\"item_cnt_month_lag1\",\"item_cnt_month_lag2\",\"item_cnt_month_lag3\",\"item_cnt_month_lag4\",\"item_cnt_month_rol1\",\"item_cnt_month_rol2\",\"item_cnt_month_rol3\",\"item_cnt_month_rol4\",\"item_cnt_month_exp1\",\"item_cnt_month_exp2\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.to_csv(\"new_base.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test,y_train,y_test = train_test_split(df0,df1, test_size=0.1,shuffle=False)\n#X_val, X_tesst,y_val,y_tesst = train_test_split(X_test,y_test, test_size=0.5,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## model\nfrom sklearn.model_selection import cross_val_score\nparam={'colsample_bytree': 0.8, 'subsample': 0.75, 'eta': 0.02, 'n_estimators': 1100, 'max_depth': 7, 'min_child_weight': 1}\nmodel = XGBRegressor(**param)\n\nmodel.fit(\n    df0, \n    df1, \n    eval_metric=\"rmse\", \n    eval_set=[(df0, df1)], \n    verbose=True, \n    early_stopping_rounds = 1)\n\npred=model.predict(df0)\nmse=mean_squared_error(df1.item_cnt_month,pred)\nrmse = math.sqrt(mse)\nprint(\"out of fold data results: \"+str(rmse))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l1=list(df1.item_cnt_month.values)\n#l1=[x if x<=1500 else 1500 for x in l1]\nl2=list(pred)\n#l2=[x if x<=1500 else 1500 for x in l2]\nl2=[0 if x<0 else round(x) for x in l2]\nmse=mean_squared_error(l1,l2)\nrmse = math.sqrt(mse)\nprint(rmse)\nimport matplotlib.pyplot as plt\nplt.plot(l1[-100:])\nplt.plot(l2[-100:],'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot feature importance\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nplot_importance(model)\npyplot.show()\n'''\n#results\nbase results test rmse: 100,cv score: -148, std cv: 8.81,  features added: keyz\nexperience 1 results test rmse: 80,cv score: -107, std cv: 9,  features added: keyz, item_cnt_month_lag1\nexperience 2 results test rmse: 76,cv score: -104.75, std cv: 13,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2\nexperience 3 results test rmse: 67,cv score: -97, std cv: 10,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3\nexperience 4 results test rmse: 66.61,cv score: -97, std cv: 11,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4\nexperience 5 results test rmse: 67,cv score: -97, std cv: 11,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5\nexperience 6 results test rmse: 66.18,cv score: -97.22, std cv: 9.60,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6\nexperience 7 results test rmse: 64.4,cv score: -95.65, std cv: 11.46,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6,item_cnt_month_lag7\nexperience 8 results test rmse: 63.69,cv score: -97.45, std cv: 9.30,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6,item_cnt_month_lag7,item_cnt_month_lag8\nexperience 9 results test rmse: 65.10,cv score: -97, std cv: 10,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6,item_cnt_month_lag7,Price_agg_lag1\nexperience 10 results test rmse: 60.89,cv score:-95.80 , std cv:11 ,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6,item_cnt_month_lag7,Price_agg_lag1,Price_agg_lag2\nexperience 11 results test rmse: 63.23,cv score:-93.64 , std cv:10.84 ,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6,item_cnt_month_lag7,Price_agg_lag1,Price_agg_lag2,Price_agg_rol1,Price_agg_rol2,Price_agg_rol3,Price_agg_rol4\nexperience 12 results test rmse: 63.72,cv score:0 , std cv:0 ,  features added: keyz, item_cnt_month_lag1,item_cnt_month_lag2,item_cnt_month_lag3,item_cnt_month_lag4,item_cnt_month_lag5,item_cnt_month_lag6,item_cnt_month_lag7,Price_agg_lag1,Price_agg_lag2,Price_agg_exp1,Price_agg_exp2\n\n\nbest results : experience 10 + optuna best params + rounding : rmse: 58.99 , cv score : -88.86, std cv: 13.47 , features : keyz, 7 count lags, 2 price lags , 0.1 test validating on train+ 0 filling\n\n\n\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross val results\n#scores = cross_val_score(model, X_train, y_train,cv=5,scoring=\"neg_root_mean_squared_error\")\n#print(\"Mean cross-validation score: %.2f\" % scores.mean())\n#print(\"Std cross-validation score: %.2f\" % scores.std())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving the model\nimport pickle\npickle.dump(model, open(\"monoprix_forecaster.pickle.dat\", \"wb\"))\nmodel.save_model(\"xgbmodel\")\n'''\n# some time later...\n# load model from file\nloaded_model = pickle.load(open(\"pima.pickle.dat\", \"rb\"))\n# make predictions for test data\ny_pred = loaded_model.predict(X_test)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XT=final_comb.drop([\"item_cnt_day\"],axis=1).drop_duplicates([\"shop_id\",\"item_id\",\"item_category\"],keep='first').reset_index().drop([\"index\",\"Price\"],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the test file for the january 2019\n#\"date_block_num\",\"shop_id\",\"item_id\",\"id_struct\",\"item_category\",\"Price_agg\",\"keyz\",\"item_cnt_month_lag1\",\"item_cnt_month_lag2\",\"item_cnt_month_lag3\",\"item_cnt_month_lag4\",\"item_cnt_month_lag5\",\"item_cnt_month_lag6\",\"item_cnt_month_lag7\",\"Price_agg_lag1\",\"Price_agg_lag2\"\nXT[\"Date\"]='2019-01-01'\nXT[\"keyz\"]=2\nXT[\"Date\"]=pd.to_datetime(XT[\"Date\"])\nXT[\"date_block_num\"]=60\nXT[\"Price_agg\"]=0\nXT[\"item_cnt_month_lag1\"],XT[\"item_cnt_month_lag2\"],XT[\"item_cnt_month_lag3\"],XT[\"item_cnt_month_lag4\"],XT[\"item_cnt_month_lag5\"],XT[\"item_cnt_month_lag6\"],XT[\"item_cnt_month_lag7\"],XT[\"Price_agg_lag1\"],XT[\"Price_agg_lag2\"]=0,0,0,0,0,0,0,0,0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nimport datetime\nfrom dateutil import relativedelta\nd={'shop_id': '44', 'item_id': '44', 'item_category': '54', 'id_struct': '68', 'price': '4', 'date': '2019-01', 'period': '3'}\nperiod=int(d[\"period\"])\nshop_id=d[\"shop_id\"]=int(d[\"shop_id\"])\nitem_id=d[\"item_id\"]=int(d[\"item_id\"])\nitem_category=d[\"item_category\"]=int(d[\"item_category\"])\nid_struct=d[\"id_struct\"]=int(d[\"id_struct\"])\nprice=d[\"price\"]=float(d[\"price\"])\ndate=d[\"date\"]=d[\"date\"]\ndd={}\nlss=date.split(\"-\")\ndates=[datetime.date(int(lss[0]),int(lss[1]), 1)]\nfor i in range(period-1):\n    dates.append(dates[-1] + relativedelta.relativedelta(months=1))\ndd[\"Date\"]=dates\ndd[\"shop_id\"]=[shop_id]*period\ndd[\"item_id\"]=[item_id]*period\ndd[\"item_category\"]=[item_category]*period\ndd[\"id_struct\"]=[id_struct]*period\ndd[\"Price\"]=[price]*period\ns = pd.DataFrame(data=dd)\ns[\"Date\"]=pd.to_datetime(s[\"Date\"])\ndff=hol_df.set_index(['Date'])\ndff1=dff.groupby(pd.Grouper(freq=\"M\")).sum()\ndff2=dff1.fillna(0).reset_index()\nll1=[x.replace(day=1) for x in dff2.Date]\ndff2[\"Date\"]=ll1\ndff3=dff2.set_index(['Date'])\nkezz=[]\nfor i in s.Date:\n    try:\n        kj=int(dff3.loc[i])\n        kezz.append(kj)\n    except:\n        kj=0\n        kezz.append(kj)\ns[\"keyz\"]=kezz\nlast_block_num=int(list(new_df.date_block_num.values)[-1])\nlast_date=list(new_df.Date)[-1]\nfirst_new_date=list(s.Date)[0]\nif first_new_date>=last_date:\n    num_months = (first_new_date.year - last_date.year) * 12 + (first_new_date.month - last_date.month)\n    new_block_num=num_months+last_block_num\nelse:\n    num_months = (last_date.year - first_new_date.year) * 12 + (last_date.month - first_new_date.month)\n    new_block_num=last_block_num-num_months\nks=[new_block_num]\nfor i in range(len(s.Date)-1):\n    ks.append(ks[-1]+1)\ns[\"date_block_num\"]=ks\ns[\"item_cnt_month_lag1\"],s[\"item_cnt_month_lag2\"],s[\"item_cnt_month_lag3\"],s[\"item_cnt_month_lag4\"],s[\"item_cnt_month_lag5\"],s[\"item_cnt_month_lag6\"],s[\"item_cnt_month_lag7\"],s[\"Price_agg_lag1\"],s[\"Price_agg_lag2\"]=0,0,0,0,0,0,0,0,0\nds=new_df[[\"Date\",\"date_block_num\",\"shop_id\",\"item_id\",\"item_category\",\"id_struct\",\"Price_agg\",\"item_cnt_month\"]]\ntry:\n    grouped_XT = ds.groupby([\"shop_id\", \"item_id\"]).get_group((shop_id,item_id))\nexcept:\n    print(\"no pair in dataset\")\nXT_list=[0]*10\nfor i in range(10):\n    try:\n        XT_list[i]=list(grouped_XT.item_cnt_month.values)[-(i+1)]\n    except:\n        break\nXs_list=[0]*5\nfor i in range(5):\n    try:\n        Xs_list[i]=list(grouped_XT.Price_agg.values)[-(i+1)]\n    except:\n        break\nfor i in range(len(s.Date)):\n    s.iloc[i,8:-2]=XT_list[:7]\n    XT_list=XT_list[1:]\nfor i in range(len(s.Date)):\n    s.iloc[i,-2:]=Xs_list[:2]\n    Xs_list=Xs_list[1:]\n    \n\n#extract lags\ngrouped_XT = XT.groupby([\"shop_id\", \"item_id\"])\nXT_list=[]\nfor key,item in grouped_XT:\n    XT_g = grouped_XT.get_group(key)\n    XD=new_df.groupby([\"shop_id\", \"item_id\"]).get_group(key)\n    XT_g[\"Price_agg\"]=list(XD.Price_agg.values)[-1]\n    XT_g[\"Price_agg_lag1\"]=list(XD.Price_agg.values)[-2]\n    XT_g[\"Price_agg_lag2\"]=list(XD.Price_agg.values)[-3]\n    XT_g[\"item_cnt_month_lag1\"]=list(XD.item_cnt_month.values)[-1]\n    XT_g[\"item_cnt_month_lag2\"]=list(XD.item_cnt_month.values)[-2]\n    XT_g[\"item_cnt_month_lag3\"]=list(XD.item_cnt_month.values)[-3]\n    XT_g[\"item_cnt_month_lag4\"]=list(XD.item_cnt_month.values)[-4]\n    XT_g[\"item_cnt_month_lag5\"]=list(XD.item_cnt_month.values)[-5]\n    XT_g[\"item_cnt_month_lag6\"]=list(XD.item_cnt_month.values)[-6]\n    XT_g[\"item_cnt_month_lag7\"]=list(XD.item_cnt_month.values)[-7]\n    XT_list.append(XT_g)\nnew_XT=pd.concat(XT_list,axis=0)\nX_test=new_XT.reset_index().drop([\"Date\",\"index\"],axis=1)\nloaded_model = pickle.load(open(\"./monoprix_forecaster.pickle.dat\", \"rb\"))\n# make predictions for test data\ny_pred = loaded_model.predict(X_test[[\"date_block_num\",\"shop_id\",\"item_id\",\"id_struct\",\"item_category\",\"Price_agg\",\"keyz\",\"item_cnt_month_lag1\",\"item_cnt_month_lag2\",\"item_cnt_month_lag3\",\"item_cnt_month_lag4\",\"item_cnt_month_lag5\",\"item_cnt_month_lag6\",\"item_cnt_month_lag7\",\"Price_agg_lag1\",\"Price_agg_lag2\"]])\nimport matplotlib.pyplot as plt\nplt.plot(y_pred)\nplt.show()\nX_test[\"item_cnt_month\"]=y_pred\nX_test[\"item_cnt_month\"]=round(X_test[\"item_cnt_month\"])\nX_test[\"Date\"]='2019-01-01'\nX_test[\"Date\"]=pd.to_datetime(X_test[\"Date\"])\nX_test[[\"Date\",\"shop_id\",\"item_id\",\"Price_agg\",\"item_cnt_month\"]].to_csv(\"forecast_2019_01.csv\")\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyperparams Tuning using Optuna\n'''\nimport optuna\ndef objective(trial,data=df0,target=df1):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.1,shuffle=False)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'colsample_bynode': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1]),\n        'subsample': trial.suggest_categorical('subsample', [0.75,0.7,0.8,0.9,0.85]),\n        'eta': trial.suggest_categorical('eta', [0.01, 0.02,0.2,0.9,0.1,0.11,0.15]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [900, 1000,1100,950,1050]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,10,11,17,20]),\n        'seed': 42,\n        'min_child_weight': trial.suggest_categorical('min_child_weight', [0.5,1,10,20,100]),\n        'booster' : 'gbtree',\n        'sampling_method' : \"uniform\",\n    }\n    model = XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(train_x,train_y)],eval_metric=\"rmse\",early_stopping_rounds=1,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain1=train[[\"index\",\"date_block_num\",\"id_struct\",\"item_category\",\"shop_id\",\"item_id\",\"Price\",\"item_cnt_day\"]]\ntrain1[\"shop_id\"] = train1[\"shop_id\"].astype('category')\ntrain1[\"item_id\"] = train1[\"item_id\"].astype('category')\ntrain1[\"index\"] = train1[\"index\"].astype('category')\ntrain1[\"date_block_num\"] = train1[\"date_block_num\"].astype('category')\ntrain1[\"id_struct\"] = train1[\"id_struct\"].astype('category')\ntrain1[\"item_category\"] = train1[\"item_category\"].astype('category')\ntrain1[\"item_cnt_day\"] = train1[\"item_cnt_day\"].astype('float64')\ntrain1[\"Price\"] = train1[\"Price\"].astype('float64')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tree based\n'''\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(n_estimators=100,max_depth=10, random_state=42,verbose=1)\nregr.fit(X_train, y_train)\npred=regr.predict(X_test)\nmse=mean_squared_error(y_test.item_cnt_day,pred)\nrmse = math.sqrt(mse)\nprint(rmse)\n'''\n'''\nfrom sklearn.ensemble import AdaBoostRegressor\nregr = AdaBoostRegressor(random_state=42, n_estimators=100,learning_rate=0.3, loss='linear')\nregr.fit(X_train, y_train)\npred=regr.predict(X_test)\nmse=mean_squared_error(y_test.item_cnt_day,pred)\nrmse = math.sqrt(mse)\nprint(rmse)\n'''\n'''\nfrom catboost import CatBoostRegressor\nmodel = CatBoostRegressor(iterations=100,\n                          learning_rate=1,\n                          depth=16)\nmodel.fit(X_train, y_train)\npred=model.predict(X_test)\nmse=mean_squared_error(y_test.item_cnt_day,pred)\nrmse = math.sqrt(mse)\nprint(rmse)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time series\n'''\nfrom fbprophet import Prophet\nimport numpy as np\nprophet_df=df[[\"Date\",\"shop_id\",\"item_id\",\"item_cnt_month\"]]\nprophet_df.set_index(['shop_id', 'item_id'],inplace=True)\nrez=[]\ncount=0\nfor pair in list_of_possibilities:\n    print(count)\n    try:\n        pair_df=prophet_df.loc[pair]\n        pair_df.rename(columns={'Date': 'ds', 'item_cnt_day': 'y'},inplace=True)\n        ptrain, ptest= train_test_split(pair_df, test_size=0.2,shuffle=False)\n        m = Prophet()\n        m.fit(ptrain)\n        future=ptest[[\"ds\"]]\n        forecast = m.predict(future)\n        mse=len(ptest.y.values)*mean_squared_error(round(forecast.yhat),ptest.y.values)\n        #rmse = math.sqrt(mse)\n        rez.append(mse)\n        count=count+1\n    except ValueError:\n        rez.append(0)\n        print(\"THIS PAIR HAS ONLY 2 DATA POINTS OR LESS: \"+str(pair))\n        count=count+1\nprint(math.sqrt(np.sum(rez)/len(prophet_df.item_cnt_day.values)))\n\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\narima_df=df[[\"Date\",\"shop_id\",\"item_id\",\"item_cnt_day\"]]\narima_df.set_index(['shop_id', 'item_id'],inplace=True)\npair_df=arima_df.loc[pair]\npair_df[\"Date\"]=pd.to_datetime(pair_df[\"Date\"])\nstart=pair_df.Date.values[0]\nend=pair_df.Date.values[-1]\nl=str(start).split('-')\nll=str(end).split('-')\nstart=l[0]+\"-\"+l[1]+\"-\"+l[2][:2]\nend=ll[0]+\"-\"+ll[1]+\"-\"+ll[2][:2]\nd={'Date':pd.date_range(start=start,end=end)}\ntemp_df = pd.DataFrame(data=d)\ncomb = pd.merge(temp_df,pair_df , on=[\"Date\"], how=\"left\")\ncomb = comb.fillna(0)\natrain, atest= train_test_split(pair_df, test_size=0.2,shuffle=False)\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(list(atrain.item_cnt_day.values), order=(5,1,0))\nmodel_fit = model.fit()\noutput = model_fit.predict(start=len(atrain.item_cnt_day.values)+1,end=len(atrain.item_cnt_day.values)+len(atest.item_cnt_day.values))\nmse = len(atest.item_cnt_day.values)*mean_squared_error(atest.item_cnt_day.values, output)\nrez.append(mse)\nprint(math.sqrt(np.sum(rez)/len(arima_df.item_cnt_day.values)))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fastai\n'''\nfrom fastai.tabular.all import *\nto = TabularPandas(train1, procs=[Categorify,Normalize],\n                   cat_names = [\"index\",\"date_block_num\",\"id_struct\",\"item_category\",\"shop_id\",\"item_id\"],\n                   cont_names=[\"Price\"],\n                   y_names='item_cnt_day')\ndls = to.dataloaders(bs=64)\ndls.show_batch()\nlearn = tabular_learner(dls, metrics=rmse)\nlearn.fit_one_cycle(1)\ntest_df = test1.copy()\ntest_df.drop(['item_cnt_day'], axis=1, inplace=True)\ndl = learn.dls.test_dl(test_df)\nl=learn.get_preds(dl=dl)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data preprocessing\ndef create_trainable_dataset(df):\n    last_block_num=int(list(base.date_block_num.values)[-1])\n    last_date=list(base.Date)[-1]\n    first_new_date=list(df.Date)[0]\n    if first_new_date>=last_date:\n        num_months = (first_new_date.year - last_date.year) * 12 + (first_new_date.month - last_date.month)\n        new_block_num=num_months+last_block_num\n    else:\n        num_months = (last_date.year - first_new_date.year) * 12 + (last_date.month - first_new_date.month)\n        new_block_num=last_block_num-num_months\n\n    l=[]\n    old=1\n    #change count\n    count=new_block_num\n    for i in df.Date:\n        if float(i.month)!=float(old):\n            old=float(i.month)\n            count=count+1\n        l.append(count)\n    df[\"date_block_num\"]=l\n\n    daata=df\n\n    #add empty dates\n    start=daata.Date.values[0]\n    end=daata.Date.values[-1]\n    l=str(start).split('-')\n    ll=str(end).split('-')\n    start=l[0]+\"-\"+l[1]+\"-\"+l[2][:2]\n    end=ll[0]+\"-\"+ll[1]+\"-\"+ll[2][:2]\n    d={'Date':pd.date_range(start=start,end=end)}\n    temp_df = pd.DataFrame(data=d)\n\n    temp=daata[[\"shop_id\",\"item_id\",\"item_category\",\"id_struct\"]].drop_duplicates([\"shop_id\",\"item_id\"]).reset_index()\n\n    temp=temp.drop([\"index\"],axis=1)\n\n    temp1=daata.drop_duplicates([\"item_id\"],keep='last').reset_index()\n    temp2=temp1[[\"item_id\",\"Price\"]].set_index([\"item_id\"])\n\n    temp['key'] = 0\n    temp_df['key'] = 0\n\n    df_cartesian = temp_df.merge(temp, how='outer')\n\n    combination=df_cartesian.drop([\"key\"],axis=1)\n\n    temppp=daata.drop([\"date_block_num\",\"item_category\",\"id_struct\"],axis=1)\n\n    final_comb=pd.merge(combination,temppp , on=[\"Date\",\"shop_id\",\"item_id\"], how=\"left\")\n\n    lis=list(final_comb.Price.fillna(-1).values)\n    lis1=list(final_comb.item_id.values)\n\n    kk=[lis[x] if lis[x]!=-1 else float(temp2.loc[lis1[x]]) for x in range(len(lis))]\n\n    final=final_comb.drop([\"Price\"],axis=1)\n\n    final[\"Price\"]=kk\n    final=final.fillna(0)\n\n    temp_df.reset_index(inplace=True)\n    temp_df=temp_df.drop([\"key\"],axis=1)\n\n    finals=pd.merge(final,temp_df , on=[\"Date\"], how=\"left\")\n\n    #generating new features\n    #holy\n    holy=pd.read_csv(\"../input/tunisian-holidays-2014-2019/holidays.csv\")\n    holy[\"date\"] = holy[\"year\"].astype(str).str.cat(holy[[\"month\", \"day_of_month\"]].astype(str), sep=\"-\")\n    holy[\"Date\"]=pd.to_datetime(holy[\"date\"])\n    holy=holy.drop([\"date\",\"year\",\"month\",\"day_of_month\"],axis=1)\n    holy[\"key\"]=1\n    finals1=pd.merge(finals,holy , on=[\"Date\"], how=\"left\")\n    finals1=finals1.fillna(0)\n\n    finals1[\"holidays\"] = finals1[\"holiday\"].astype('category')\n    finals1[\"holidayz\"] = finals1[\"holidays\"].cat.codes\n    finals2=finals1.drop([\"holidays\",\"holidayz\",\"holiday\"],axis=1)\n\n    #monthly_Agg\n    l=[]\n    old=1\n    #change count\n    count=new_block_num\n    for i in finals2.Date:\n        if float(i.month)!=float(old):\n            old=float(i.month)\n            count=count+1\n        l.append(count)\n    finals2[\"date_block_num\"]=l\n    finals2[\"item_cnt_month\"] = finals2.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"item_cnt_day\"].transform(np.sum)\n    finals2[\"Price_agg\"] = finals2.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"Price\"].transform(np.mean)\n    finals2[\"keyz\"] = finals2.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"key\"].transform(np.sum)\n    del finals2[\"item_cnt_day\"]\n    del finals2[\"Price\"]\n    del finals2[\"index\"]\n    del finals2[\"key\"]\n\n    finals2 = finals2.drop_duplicates([\"date_block_num\", \"shop_id\", \"item_id\"])\n\n    finals2.reset_index(inplace=True)\n    finals2.drop([\"index\"],axis=1,inplace=True)\n\n    #feature engineering lag roll expand\n    grouped_df = finals2.groupby([\"shop_id\", \"item_id\"])\n    df_list=[]\n    for key,item in grouped_df:\n        df_g = grouped_df.get_group(key)\n        #lagg features for price and count\n        #count\n        df_g[\"item_cnt_month_lag1\"]=df_g.item_cnt_month.shift(1).fillna(0)\n        df_g[\"item_cnt_month_lag2\"]=df_g.item_cnt_month.shift(2).fillna(0)\n        df_g[\"item_cnt_month_lag3\"]=df_g.item_cnt_month.shift(3).fillna(0)\n        df_g[\"item_cnt_month_lag4\"]=df_g.item_cnt_month.shift(4).fillna(0)\n        df_g[\"item_cnt_month_lag5\"]=df_g.item_cnt_month.shift(5).fillna(0)\n        df_g[\"item_cnt_month_lag6\"]=df_g.item_cnt_month.shift(6).fillna(0)\n        df_g[\"item_cnt_month_lag7\"]=df_g.item_cnt_month.shift(7).fillna(0)\n        #df_g[\"item_cnt_month_lag8\"]=df_g.item_cnt_month.shift(8).fillna(method='bfill')\n        #price\n        df_g[\"Price_agg_lag1\"]=df_g.Price_agg.shift(1).fillna(0)\n        df_g[\"Price_agg_lag2\"]=df_g.Price_agg.shift(2).fillna(0)\n        df_list.append(df_g)\n    new_df=pd.concat(df_list,axis=0)\n\n    new_df.reset_index(inplace=True)\n    new_df.drop([\"index\"],axis=1,inplace=True)\n    return new_df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0=new_df[[\"date_block_num\",\"shop_id\",\"item_id\",\"id_struct\",\"item_category\",\"Price_agg\",\"keyz\",\"item_cnt_month_lag1\",\"item_cnt_month_lag2\",\"item_cnt_month_lag3\",\"item_cnt_month_lag4\",\"item_cnt_month_lag5\",\"item_cnt_month_lag6\",\"item_cnt_month_lag7\",\"Price_agg_lag1\",\"Price_agg_lag2\"]]\ndf1=new_df[[\"item_cnt_month\"]]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test,y_train,y_test = train_test_split(df0,df1, test_size=0.1,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some time later...\n# load model from file\nloaded_model = pickle.load(open(\"./monoprix_forecaster.pickle.dat\", \"rb\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## model\nparam={'colsample_bytree': 0.8, 'subsample': 0.75, 'eta': 0.02, 'n_estimators': 1100, 'max_depth': 7, 'min_child_weight': 1}\nmodel = XGBRegressor(**param)\n\nmodel.fit(\n    df0, \n    df1, \n    eval_metric=\"rmse\", \n    eval_set=[(df0, df1)], \n    verbose=True, \n    early_stopping_rounds = 1,xgb_model=\"./xgbmodel\")\n\npred=model.predict(df0)\nmse=mean_squared_error(df1.item_cnt_month,pred)\nrmse = math.sqrt(mse)\nprint(\"out of fold data results: \"+str(rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_model(\"./xgbmodel\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}